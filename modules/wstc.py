import sys
import os
import torch
import torch.nn as nn

from wsam import WSAM
from tcdam import TCDAM
from text_projector import SmallMLPProjector
from meta_conditioning import MetaConditioning
from cross_modal import CrossModalFusion

TASK_DIM = 256              # âžŸ æƒ³æ”¹ 512 ä¹Ÿè¡Œï¼Œä½†ä¸‰å¤„è¦ä¸€è‡´
PROJ_DIM = 256

import torch.nn.functional as F

def temperature_scaling(logits, temperature=1.0):
    """åº”ç”¨æ¸©åº¦ç¼©æ”¾è°ƒæ•´è¾“å‡º"""
    logits = logits / temperature
    return F.softmax(logits, dim=-1)


class UniMedCLIP_WSTC(nn.Module):
    def __init__(self,
                 base_model,
                 # clip_model,
                 num_classes: int = 15,
                 task_dim: int = TASK_DIM,
                 proj_dim: int = PROJ_DIM,
                 *,
                 use_wsam: bool = True,
                 use_tcdam: bool = True,
                 with_cls_head: bool = True,
                 task_id = None,
                 use_cross_modal: bool = False
                 ):
        super().__init__()
        self.base = base_model
        self.clip = base_model
        self.use_wsam = use_wsam
        self.use_tcdam = use_tcdam
        self.with_cls = with_cls_head
        self.task_id = task_id
        num_tasks = 10  # âœ… ä½ ç›®å‰æœ€å¤šæ”¯æŒå‡ ä¸ªä»»åŠ¡ï¼ˆå¯åŽç»­ç”¨ configï¼‰
        self.task_embed = nn.Embedding(num_tasks, task_dim)
        self._skip_proj = False  # True â‡’ ç›´æŽ¥ç”¨ 512 ç»´ CLS / Text
        # self.use_cross_modal = use_cross_modal  # æ–°å¢žä¸€ä¸ª flag
        # self.cross_fuser = CrossModalFusion(dim=proj_dim)

        feat_dim = self.clip.visual.output_dim  # typically 512
        # âœ… å†»ç»“åŽŸå§‹ CLIP æ¨¡åž‹
        for p in self.clip.parameters():
            p.requires_grad_(False)

        if hasattr(self.base, "text_encoder"):  # HuggingFace BERT
            text_emb_dim = self.base.text_encoder.config.hidden_size  # é€šå¸¸ 768
        else:  # open-clip CLIP
            text_emb_dim = (self.clip.text_projection.out_features
                            if hasattr(self.clip, "text_projection")
                            else feat_dim)  # é€šå¸¸ 512

        # âœ… åˆå§‹åŒ– WSAM æ¨¡å—
        if use_wsam:
            self.wsam = WSAM(image_feature_dim=feat_dim, task_embedding_dim=task_dim)

        # âœ… åˆå§‹åŒ– TCDAM æ¨¡å—
        if use_tcdam:
            self.tcdam = TCDAM(task_embedding_dim=task_dim, image_feature_dim=feat_dim, projection_dim=proj_dim)
            self.image_proj = nn.Identity()
            feat_dim = self.clip .visual.output_dim

            self.text_proj = SmallMLPProjector(  # 768â†’256 / 512â†’256
                input_dim=text_emb_dim,
                hidden_dim=text_emb_dim,
                output_dim=proj_dim
            )
        else:
            self.image_proj = nn.Identity()
            # self.text_proj = nn.Identity()
            # ---- WSAM-only éœ€è¦æŠŠ 768 â†’ 512ï¼ˆä¸Žè§†è§‰å¯¹é½ï¼‰ ----
            if text_emb_dim != feat_dim:  # 768 vs 512
                self.text_proj = nn.Linear(text_emb_dim, feat_dim, bias=False)
                torch.nn.init.normal_(self.text_proj.weight, std=0.02)
            else:
                self.text_proj = nn.Identity()

        # âœ… åˆ†ç±»å¤´ï¼ˆå¯é€‰ï¼‰
        out_dim = proj_dim if use_tcdam else task_dim if use_wsam else feat_dim

        if with_cls_head:
            assert num_classes is not None, "with_cls_head=True ä½†æœªè®¾ç½® num_classes"
            self.cls = nn.Linear(out_dim, num_classes)

        print('task_id', task_id)
        if task_id in [3, 8]:  # monuseg åˆ†å‰²ä»»åŠ¡
            self.seg_decoder = nn.Sequential(
                nn.Linear(out_dim, 256),
                nn.ReLU(),
                nn.Unflatten(1, (16, 4, 4)),  # 256 â†’ [16, 4, 4]
                nn.Upsample(scale_factor=2),  # [16,8,8]
                nn.Conv2d(16, 8, 3, padding=1),
                nn.ReLU(),
                nn.Upsample(scale_factor=4),  # [8,32,32]
                nn.Conv2d(8, 1, 1),
                nn.Sigmoid()
            )

        self.l2 = nn.functional.normalize
        self.fallback_proj = nn.Linear(feat_dim, task_dim, bias=False)

        # âœ… æ‰€æœ‰æ¨¡å—ç§»åŠ¨åˆ° CLIP çš„è®¾å¤‡ä¸Š
        tgt_device = next(self.clip.parameters()).device
        self.to(tgt_device)
        if self.text_proj is not None:
            self.text_proj = self.text_proj.to(tgt_device)

        self.meta_condition = MetaConditioning(task_dim=task_dim, num_tasks=num_tasks)
        # print("[INFO] feat_dim:", feat_dim)
        print("CLIP visual backbone type:", type(self.clip.visual))

    @property
    def text_encoder(self):
        return self.base.text_encoder  # è®©å¤–éƒ¨èƒ½ç›´æŽ¥è®¿é—®

    @property
    def encode_text(self):  # å…¼å®¹ open_clip æŽ¥å£
        return self.base.encode_text

    @property
    def tokenizer(self):
        return getattr(self.base, "tokenizer", None)

    def forward(self, pixel_values, question_ids: torch.LongTensor = None,
            question_mask: torch.LongTensor = None,
            answer_ids: torch.LongTensor   = None,
            answer_mask: torch.LongTensor  = None):
        """
        pixel_values : [B, 3, H, W]
        return       : logits  [B, num_classes]   (with_cls=True)
                      or       image_feats       (otherwise)
        """
        dbg = hasattr(self.clip.visual, "forward_features")
        # print(f"[DBG] new_interface={dbg}")

        with torch.no_grad():
            vis_dict = self.clip.encode_image(pixel_values)  # â† ä¸ä¼  flag
            cls_token = vis_dict["pooled_output"]  # [B,512]
            patch_tokens = vis_dict["patch_tokens"]  # [B,P,512] or None

            # â‘¡ è‹¥ use_wsam ä½† patch_tokens ä»ä¸º Noneï¼Œç›´æŽ¥æŠ¥é”™
            if self.use_wsam and patch_tokens is None:
                raise RuntimeError(
                    "patch_tokens is None â€” backbone æ²¡è¿”å›ž patchã€‚"
                    "è¯·ç¡®è®¤ open_clip ç‰ˆæœ¬ â‰¥ 2.23 æˆ–æ¢ç”¨ hook æ–¹æ¡ˆã€‚")

            # â‘¢ patch_tokens å¯èƒ½æ˜¯ [B,D] â†’ ç»Ÿä¸€æˆ [B,1,D]
            if patch_tokens is not None and patch_tokens.ndim == 2:
                patch_tokens = patch_tokens.unsqueeze(1)
            # ---------- â‘  å–å¾—è§†è§‰è¾“å‡º ----------
            elif isinstance(vis_dict, dict):
                def pick(d, *keys):
                    for k in keys:
                        v = d.get(k, None)
                        if v is not None:  # åªçœ‹ Noneï¼Œä¸å¯¹å¼ é‡åš bool è¿ç®—
                            return v
                    return None

                cls_token = pick(vis_dict,
                                 "image_features",
                                 "pooled_output",
                                 "pooled",
                                 "cls_embedding")
                patch_tokens = pick(vis_dict, "patch_tokens", "x")
            else:
                def pick_attr(obj, *names):
                    for n in names:
                        v = getattr(obj, n, None)
                        if v is not None:
                            return v
                    return None

                cls_token = pick_attr(vis_dict,
                                      "pooled_output",
                                      "pooler_output",
                                      "last_hidden_state")
                # last_hidden_state â†’ [B,N,D]ï¼Œå– CLS
                if cls_token is not None and cls_token.ndim == 3:
                    cls_token = cls_token[:, 0, :]

                patch_tokens = pick_attr(vis_dict, "patch_tokens", "x")

            # è‹¥ä»æ‹¿ä¸åˆ° CLSï¼Œåˆ™ç”¨ patch[0] é€€è·¯
            if cls_token is None and patch_tokens is not None and patch_tokens.ndim == 3:
                cls_token, patch_tokens = patch_tokens[:, 0, :], patch_tokens[:, 1:, :]
            # è‹¥è¿˜æ˜¯æ‹¿ä¸åˆ° CLSï¼Œå°±ç›´æŽ¥æŠŠ patch_tokens çš„ç¬¬ 1 ä¸ª token å½“ CLS
            if cls_token is None and patch_tokens is not None and patch_tokens.ndim == 3:
                cls_token, patch_tokens = patch_tokens[:, 0, :], patch_tokens[:, 1:, :]

            # æ ‡å‡†åŒ–å½¢çŠ¶
            if patch_tokens is not None and patch_tokens.ndim == 2:  # [B,D] â†’ [B,1,D]
                patch_tokens = patch_tokens.unsqueeze(1)
            if cls_token.ndim == 3:  # [B,1,D] â†’ [B,D]
                cls_token = cls_token.squeeze(1)


            # å…¨éƒ¨æ¬å›žåŒä¸€è®¾å¤‡
            cls_token = cls_token.to(pixel_values.device)
            if patch_tokens is not None:
                patch_tokens = patch_tokens.to(pixel_values.device)

            cls_token = self.image_proj(cls_token)  # [B,512]
            if patch_tokens is not None:
                patch_tokens = self.image_proj(patch_tokens)  # [B,L,512]

        if patch_tokens is not None and patch_tokens.shape[0] != cls_token.shape[0]:
            # hook æ¥è‡ª blockï¼Œå½¢çŠ¶æ˜¯ [Seq, B, D]ï¼ŒæŠŠå‰ä¸¤ç»´æ¢å›žæ¥
            patch_tokens = patch_tokens.transpose(0, 1)  # â†’ [B, Seq, D]
        # ---------- â‘¢ WSAM / TCDAM åˆ†æ”¯ ----------
        if self.use_wsam:
            task_feat = self.wsam(patch_tokens)  # [B, task_dim]
            task_feat = self.meta_condition(task_feat, task_id=self.task_id)
            task_feat = task_feat.mean(dim=1) if task_feat.ndim == 3 else task_feat
            if task_feat.ndim == 3:
                task_feat = task_feat.mean(dim=1)
            if task_feat.ndim == 3:
                task_feat = task_feat.mean(dim=1)

        else:
            task_feat = self.text_proj(cls_token)

        if self.use_tcdam:
            proj = self.tcdam(task_feat)
            image_feats = torch.bmm(cls_token.unsqueeze(1), proj).squeeze(1)
            image_feats = self.image_proj(image_feats)  # [B, proj_dim]
        else:
            image_feats = cls_token

        image_feats = self.l2(image_feats, dim=-1)  # L2-norm
        # å¦‚æžœä¼ å…¥äº† question_idsï¼Œå°±åš VQA æµç¨‹
        # â”€â”€â”€ åœ¨æœ€å°¾éƒ¨ï¼ŒæŠŠçŽ°æœ‰çš„ â€œif question_ids â€¦â€ å…¨åˆ ï¼Œæ¢æˆï¼š
        if question_ids is not None and self.use_cross_modal:
            print("[DEBUG] ðŸ”¥ Running cross-modal fusion")
            vis = self.clip.encode_image(pixel_values, return_patch_tokens=True)
            cls_tok = vis["image_features"]
            patch_tokens = vis["patch_tokens"]


            # 1) æ‹¿ pooled + patch tokens
            # if hasattr(self.clip.visual, "forward_features"):
            #     vis = self.clip.visual.forward_features(
            #         pixel_values, return_patch_tokens=True
            #     )
            #     cls_tok = vis["pooled_output"]
            #     patch_tokens = vis["patch_tokens"]
            # else:
            #     # HF Transformers CLIP æ²¡æœ‰ forward_featuresï¼Œåªèƒ½ plain forward
            #     vis = self.clip.visual(pixel_values)  # ä¸è¦ return_dict
            #     if isinstance(vis, torch.Tensor):
            #         # æžæ—§æŽ¥å£ç›´æŽ¥ç»™ [B,D] æˆ– [B,1,D]
            #         cls_tok = vis.squeeze(1) if vis.ndim == 3 else vis
            #     else:
            #         # HF æœ‰å¯èƒ½è¿”å›žä¸€ä¸ª ModelOutput
            #         cls_tok = getattr(vis, "pooler_output", None) \
            #                   or vis.last_hidden_state[:, 0]
            #     patch_tokens = None
            # [B,512]         [B,P,512] æˆ– [B*P,512]
            if patch_tokens.ndim == 2:
                patch_tokens = patch_tokens.view(
                    cls_tok.size(0), -1, cls_tok.size(1)
                )
            # 2) æŠ•åˆ° proj_dimï¼ˆåŒ text_proj è¾“å‡ºç»´åº¦ï¼‰
            patch_tokens = self.patch_proj(patch_tokens)  # [B,P,256]
            q_feat = self.base.text_encoder(
                question_ids, attention_mask=question_mask
            ).pooler_output  # [B,512]
            q_feat = self.text_proj(q_feat)  # [B,256]
            # 3) crossâ€modal fuse
            q_seq = q_feat.unsqueeze(1)  # [B,1,256]
            fused = self.cross_fuser(q_seq, patch_tokens)  # [B,1,256]
            q_feat = fused.squeeze(1)  # [B,256]
            # 4) ç­”æ¡ˆ embedding
            out_a = self.clip.text_encoder(
                answer_ids, attention_mask=answer_mask
            ).pooler_output  # [A,512]
            ans_emb = self.text_proj(out_a)  # [A,256]
            # 5) joint & logits
            image_feats = self.l2(self.image_proj(cls_tok), dim=-1)  # [B,256]
            joint = image_feats * q_feat
            return joint @ ans_emb.T

        # return self.cls(image_feats) if self.with_cls else image_feats
        if self.with_cls and self.task_id not in [3, 8]:  # éž segmentation ä»»åŠ¡ï¼ˆå¦‚ ChestXrayã€PCamï¼‰
            return self.cls(image_feats)
        elif self.task_id in [3, 8]:  # 3 = monusegï¼Œåˆ†å‰²ä»»åŠ¡
            seg_logits = self.seg_decoder(image_feats)  # [B, 1024]
            seg_mask = seg_logits.view(-1, 1, 32, 32)  # â†’ [B,1,32,32]
            seg_mask = nn.functional.interpolate(seg_mask, size=(256, 256), mode="bilinear", align_corners=False)

            return seg_mask
        else:
            return image_feats


    def forward_seg(self, pixel_values):
        return self.forward(pixel_values)

    @torch.no_grad()
    def encode_image(self,
                     pixel_values,
                     prompt_ids: torch.LongTensor = None,
                     prompt_mask: torch.LongTensor = None):
        """
        å¦‚æžœ prompt_ids ä¸ä¸º Noneï¼Œå°±åš cross-modal èžåˆåŽè¿”å›žæ–°çš„ image_featsï¼›
        å¦åˆ™ï¼Œæ²¿ç”¨åŽŸæ¥ WSAM/TCDAM â†’ L2-norm æµç¨‹ã€‚
        """
        if getattr(self, "_skip_proj", False):
            out = self.clip.encode_image(pixel_values)
            cls = out["pooled_output"] if isinstance(out, dict) else out
            return self.l2(cls.float(), dim=-1)

        # 1) å–è§†è§‰è¾“å‡º
        if hasattr(self.clip.visual, "forward_features"):
            vis = self.clip.visual.forward_features(pixel_values, return_patch_tokens=True)
            cls_token = vis["pooled_output"]
            patch_tokens = vis["patch_tokens"]
        else:
            # â€”â€” è€æŽ¥å£ï¼šå¯èƒ½è¿”å›ž Tensor / dict / ModelOutput / NamedTuple
            out = self.clip.visual(pixel_values)

            if isinstance(out, torch.Tensor):  # â‘  ç›´æŽ¥ç»™ CLS
                cls_token = out.squeeze() if out.ndim == 3 else out  # [B,D]
                patch_tokens = None
            else:  # â‘¡ dict / ModelOutput
                cls_token = (
                        getattr(out, "pooler_output", None) or
                        getattr(out, "pooled_output", None) or
                        getattr(out, "cls_embedding", None) or
                        getattr(out, "last_hidden_state", None)[:, 0]
                )
                patch_tokens = (
                        getattr(out, "patch_tokens", None) or
                        getattr(out, "x", None)
                )

            # è‹¥ patch_tokens æ˜¯ [B,D] â†’ [B,1,D]
            if patch_tokens is not None and patch_tokens.ndim == 2:
                patch_tokens = patch_tokens.unsqueeze(1)

        # å¦‚æžœ patch_tokens æ˜¯ [B*P, D], æ¢å¤æˆ [B,P,D]
        if patch_tokens is not None and patch_tokens.ndim == 2:
            B = cls_token.size(0)
            D = cls_token.size(-1)
            patch_tokens = patch_tokens.view(B, -1, D)

        # 2) å…ˆæŠŠ pooled cls_token æŠ•åˆ° proj_dimï¼ˆå’Œ text_proj ä¸€è‡´ï¼‰
        #    è¿™æ ·åŽé¢ cross_fuser å’Œ answer/text_proj è¾“å‡ºèƒ½å¯¹ä¸Š
        cls_proj = self.image_proj(cls_token)    # [B, proj_dim]
        patches  = self.image_proj(patch_tokens) if patch_tokens is not None else None  # [B,P,proj_dim]

        # â€”â€” å¦‚æžœæ²¡ç»™ promptï¼Œå°±å›žé€€åˆ°åŽŸæ¥çš„ WSAM/TCDAM åˆ†æ”¯ â€”â€”
        if prompt_ids is None:
            # ---------- æ—  promptï¼šåˆ†ç±» / æ£€ç´¢ ----------
            if self.use_wsam and patches is not None:
                task_feat = self.wsam(patches)  # [B,256]
                print("after WSAM", task_feat.shape)
                task_feat = self.meta_condition(task_feat, self.task_id)
                task_feat = task_feat.mean(1) if task_feat.ndim == 3 else task_feat
            else:
                # æ‹¿ä¸åˆ° patch_tokens â†’ ç”¨ fallback_proj æŠŠ CLS(768) åŽ‹åˆ° 256
                task_feat = self.fallback_proj(cls_token)  # [B,256]
            if self.use_tcdam:
                proj = self.tcdam(task_feat)  # [B,512,256]
                image_feats = torch.bmm(cls_proj.unsqueeze(1), proj).squeeze(1)
            else:
                image_feats = cls_proj

            return self.l2(image_feats, dim=-1)

        # â€”â€” 3) æœ‰ prompt æ—¶ï¼Œåš cross-modal â€”â€”
        # 3.1) text encode + project
        out_q = self.clip.text_encoder(
            prompt_ids,
            attention_mask=prompt_mask
        )
        q_feat = (out_q.pooler_output
                  if hasattr(out_q, "pooler_output") else
                  out_q.last_hidden_state[:,0])       # [B, D_vis]
        q_feat = self.text_proj(q_feat)            # [B, proj_dim]

        # 3.2) crossâ€modal fuse
        q_seq = q_feat.unsqueeze(1)                # [B,1,proj_dim]
        # patches: [B,P,proj_dim]
        fused = self.cross_fuser(q_seq, patches)   # [B,1,proj_dim]
        q_feat = fused.squeeze(1)                  # [B, proj_dim]

        # 3.3) ç›´æŽ¥æŠŠ q_feat å½“ image_feats è¿”å›ž
        #      ï¼ˆä¹Ÿå¯ä»¥åš elementâ€wise æˆ– concatï¼Œçœ‹ä½ æƒ³è¦æ€Žæ ·åˆ©ç”¨ï¼‰
        return self.l2(q_feat, dim=-1)

    @torch.no_grad()
    def encode_text(self, token_ids):
        if getattr(self, "_skip_proj", False):
            txt = self.clip.encode_text(token_ids)
            return self.l2(txt.float(), dim=-1)
        txt = self.clip.encode_text(token_ids)  # åŽŸå§‹ 512 ç»´
        txt = self.text_proj(txt)  # -> 256 ç»´
        return self.l2(txt, dim=-1)
